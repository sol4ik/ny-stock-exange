---
title: "Tesla, Inc. Stocks Research"
author: "Solomiia Lenio   Mykola Morhunenko"
date: "January 2020"
output: pdf_document
---
```{r, echo=FALSE, message=FALSE}
require(ggplot2)
library(ggplot2)
require(moments)
library(moments)
require(fitdistrplus)
library(fitdistrplus)
```
## Task description
Nowadays it is quite popular though that any person can make money on stock exchange - all you need is starter money and some knowledge.  
We decided to check if it is possible to get a big revenue from some big companies without much effort.  

## Data description
For our research we first wanted to use the [**New York Stock Exchange dataset**](https://www.kaggle.com/dgawlik/nyse) that consists of the historical data on New York Stock Exchange from 2012 until 2016 parced from Yahoo! Finance. But since tha data are quite outdated and we do not need the data on all the companies, we decided to get similar data.  
For our final dataset we downloaded the historical data on **Tesla, Inc.** stocks from Yahoo! Finanacefrom January 5, 2015 until January 4, 2020.  
You can find the .csv file created [**here**](https://github.com/sol4ik/tsla-stocks-research/blob/master/TSLA-5.csv).  
We're going to load the data and see what we have.  
```{r, message=FALSE}
tsla.data <- read.csv("TSLA-5.csv")
head(tsla.data)
```
Here we have a data frame consisting of **7 different columns** and **1259** rows. What do these data mean?  
Each row in tha data frame represent a day, the stock was out on a market. The date of the day can be found in the **date** column, consisting of date objects.  
All the follwoing columns, except for the last one, tell us about the **price** of the stock. The **open** and **close** columns tell the price of the stock when the market opened and closed on that day. The **low** and **high** columns consist of the lowest and the highest values the stock price reached during the day. **Adj.Close** column consists of the close price adjusted for both dividends and splits, although it almost always is the same as the close one.  
The last column - **volume** - consists of integer values refering to the numbers of shares that have been bought and sold for the day.  
   
## Data analysis
Let's first check how Tesla stock prices behaved themselves during the last five years.  
For a better look of the plot we're going to plot every 6$^{th}$ day data, we'll show later why it doesn't change much.  
```{r, echo=FALSE, message=FALSE}
graph.data <- data.frame("Date"=0, "Close"=0)
dates <- tsla.data$Date
closes <- tsla.data$Close
tmp <- 0
for (index in seq(1, nrow(tsla.data), by=6)) {
  tmp <- tmp + closes[index]
  graph.data <- rbind(graph.data, list(dates[index], tmp / 6)) 
  tmp <- 0
}
graph.data <- graph.data[-1, ]

ggplot(graph.data) +
  geom_line(aes(x = seq(1:nrow(graph.data)), y = Close),
            color="blue") +
  labs(x = "Time, day", y="Stock price, $", title="Tesla, Inc. stock prices",
       subtitle="based on data from Jan.5, 2015 until Jan.4, 2020") + 
   theme(axis.text.x=element_blank(),
         axis.ticks.x=element_blank(),
         axis.ticks.y=element_blank())
```
As we can see here, the stock prices have significantly increased during last months. Let's then check if we can make some good money here :)  
  
### Daily return
Since we wanted to check if we can make easy money on Tesla we'll test one of the easiest way - buying the shares one day and selling them the next day. Even easier way that does not envolve any tracking - buying and selling at the same time.  
For our case let's take the buying and selling time to be just before the market closes.  
For this we'll need a **daily return** notion - the amount of stock price daily growth. Daily return is calculated by a simple intuitive formula
$$R_t =  \frac{C_t - C_{t-1}}{C_{t-1}} = \frac{C_t}{C_{t-1}} - 1$$
```{r}
tsla.data$PrevClose <- tsla.data$Close
# since there is no previous day for the first day, we'll get NA value
# in order not to get NA in further calculations we'll get rid of the first day
tsla.data <- na.omit(transform(tsla.data, PrevClose = c(NA, PrevClose[-nrow(tsla.data)])))
tsla.data$DailyRet <- with(tsla.data, Close / PrevClose - 1)

head(tsla.data)
```
Now we'll see what we can say on the daily return values we got.  
We can start off with some simple numeric characterics.
```{r}
cat("mean value: ", mean(tsla.data$DailyRet), "  ")
cat("max value: ", max(tsla.data$DailyRet), "  ")
cat("min value: ", min(tsla.data$DailyRet), "\n")
cat("standart deviation: ", sd(tsla.data$DailyRet))
```
Looking at these numbers we already can say that day-to-day changes in the stock prices are not that significant. Small standart deviation tells us that variety of all the values is quite small meaning that day-to-day buying-selling is not a risky strategy.  
It drives us to a conculsion that the strategy of selling the shares just the next day is **not the best strategy for a good money income, although it's a safe one** - you will not loose much if something goes wrong.  
   
### Annual return
Still, daily return can give even more information on some company stock prices - daily return can be used to derrive **annual return**.   
Annual return is the approxiamation of finance return in a year. It can be calucated with a single daily return value by the following formula
$$Y_t = (R_t + 1)^{365} - 1$$
We're now going to add one more column with calculated annual return.
```{r, echo=FALSE, message=FALSE}
power <- function(x, n) {
  res <- 1
  for (i in 1:n) {
    res <- res * x
  }
  return(res)
}
```

```{r}
tsla.data$YearRet <- with(tsla.data, power(DailyRet + 1, 365) - 1)
head(tsla.data)
```
Now we can analyze what data we get.  
Again we'll start with some simple characteristics.  
```{r, echo=FALSE, message=FALSE}
mode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
mode(tsla.data$YearRet)
```
```{r}
cat("mean value: ", mean(tsla.data$YearRet), "  ")
cat("max value: ", max(tsla.data$YearRet), "  ")
cat("min value: ", min(tsla.data$YearRet))
```
The data we got seem quite interesting and not that self-explanatory.
```{r}
cat("standart deviation: ", sd(tsla.data$YearRet))
```
Standart deviation here is not that small which tells us that there is a variety in the annual return values.
```{r}
cat("skewness: ", skewness(tsla.data$YearRet), " ")
cat("kurtosis: ", kurtosis(tsla.data$YearRet))
```
Positive skewness tells us that the distribution of annual return has a heavy right-tail. In terms of the return itself it means that the values are mostly concentrated around the expected value, altough there is a small probability to get values much bigger than the expected one.  
Positive kurtosis means that the distribution has so-called thick tails. The high value of kurtosis tells about frequent extreme values in the distribution. In terms of the data we have it again means that there are a lot of extreme values for the annual return.  
  
For further analysis we'll visualize all the data we got on annual return.  
  
```{r, echo=FALSE, message=FALSE}
ggplot(tsla.data, aes(x = YearRet)) +
  geom_histogram(color="salmon3", fill="salmon1") +
  geom_vline(aes(xintercept = mean(YearRet))) +
  geom_vline(aes(xintercept = mean(YearRet) + sd(YearRet)),
             color="brown", linetype="dashed") +
  geom_vline(aes(xintercept = mean(YearRet) - sd(YearRet)),
             color="brown", linetype="dashed") +
  labs(x = "Annual return",
       title = "Histogram of annual returns on Tesla, Inc. stock prices",
       subtitle="based on data from Jan.6, 2015 until Jan.4, 2020") +
  theme(axis.title.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.text.y=element_blank())
```
As we can see the data turned to have some quite extreme values and the plot does not help much. In order to fix this we're going to take the logarithm of the annual return, since we're working with ratios, and plot the data we got.  
```{r, message=FALSE, warning=FALSE}
log.ret <- with(tsla.data, log2(YearRet))
log.ret <- log.ret[!is.nan(log.ret) & !is.infinite(log.ret)]
head(log.ret)
```
```{r, echo=FALSE, message=FALSE}
ggplot(data.frame(data = log.ret), aes(x = data)) +
  geom_histogram(aes(y=..density..), color="salmon3", fill="salmon1") +
  geom_vline(aes(xintercept = mean(data))) +
  geom_vline(aes(xintercept = mean(data) + sd(data)),
             color="brown", linetype="dashed") +
  geom_vline(aes(xintercept = mean(data) - sd(data)),
             color="brown", linetype="dashed") +
  geom_density(color="red3") +
  labs(x = "log Annual return",
       title = "Histogram of log of annual returns on Tesla, Inc. stock prices",
       subtitle="based on data from Jan.6, 2015 until Jan.4, 2020") +
  theme(axis.title.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.text.y=element_blank())
```
It would be expected to suggest that annual reurn follows a lognormal ditribution but since it assumes both positive and negative values lognormal distribution is not suitable here.  
Our next suggestion here is **logistic distribution**. By its shape it may resemble a normal distribution altough it has heavier tail - meaning higher kurtosis.    
In order to check it we're going to run **Kolmogorov-Smirnov test for goodness-of-fit**.  
  
$H_0:$ annual returns follow distribution $F$, being s logistic distribution with parameters $\mu = \hat{m_1}$ and $s = \frac{\sqrt{3}\hat{sd}}{\pi}$  
$H_1:$ annual return does not follow distribution $F$
```{r}
ks.test(tsla.data$YearRet, "plogis", location=mean(tsla.data$YearRet), scale=sqrt(3) * sd(tsla.data$YearRet) / pi)
```
p-value of the test is almost zero which means that our guess was wrong and we reject $H_0$.  
   
In order to understand the distribution we'll go back to how we calculated it.  
We used a formula $Y_t = (R_t + 1)^{365} - 1$, where $R_t$ was the value of a daily return.  
We'll plot daily return data and try to fit them to some distribution in order to get some idea of how annual return is distributed.  
```{r, echo=FALSE, message=FALSE}
ggplot(tsla.data, aes(x = DailyRet)) +
  geom_histogram(aes(y=..density..), color="gold4", fill="gold1") +
  geom_vline(aes(xintercept = mean(DailyRet))) +
  geom_vline(aes(xintercept = mean(DailyRet) + sd(DailyRet)),
             color="brown", linetype="dashed") +
  geom_vline(aes(xintercept = mean(DailyRet) - sd(DailyRet)),
             color="brown", linetype="dashed") +
  geom_density(color="darkorange4") +
  labs(x = "Daily return",
       title = "Histogram of daily returns on Tesla, Inc. stock prices",
       subtitle="based on data from Jan.6, 2015 until Jan.4, 2020") +
  theme(axis.title.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.text.y=element_blank())
```
```{r}
descdist(tsla.data$DailyRet)
```
It seems like daily returns might follow a logistic distribution.  
```{r, warning=FALSE}
fit.logis <- fitdist(tsla.data$DailyRet, "logis")
plot(fit.logis)
```
```{r}
fit.logis
```
Now we'll run a test.

$H_0:$ daily returns follow a distribution $F = \mathcal{logis}(0.001, 0.01)$
$H_1:$ daily returns do not follow distribution $F$
```{r}
ks.test(tsla.data$DailyRet, "plogis", location=fit.logis$estimate[1], scale=fit.logis$estimate[2])
```
p-value of the test is not that big, so in most cases we need to reject $H_0$. Still the plots of fitting empirical data to theoretical ones look quite good, but following Cullen and Fray graph our data are not that close to actual theoretical logisitic distribution.      
Still, there are no better estimates for the distribution, so we stick to **logistic distribution**.  
  
In order to caculate annual return we raised daily return + 1 to the power of 365, basically we did multiplying.   
Prodcut of two and more i.i.d. r.v.s never has the same distribution. In fact, it has a **product distribution** except for some cases like lognormal distribution.  
So, based on the way we calculated annual return, we make conclusion that our data on annual returns follow a **product distrubution**.  
  
#### Conclusions


### Dependences
Now we're going to check on some dependences between the data we have.  
```{r}
drops <- c("Date")
no.date <- tsla.data[ , !(names(tsla.data) %in% drops)]

dependence <- cor(no.date)
dependence
```
As we can see from the output the greatest correlation here can be seen between the values of low, high, close, open etc. columns. It was quite obvious to get such a result, since they all senote different values of the same, so to say, stock price function, that as we showed before do not change extremely in a short period of time.  
Still, we can make some use of these correlations. We can suggest a linnear relation between close and previous close price values in order to build a model for price predicitions.  
We're going to plot the data in order to see the actual dependency between them.  
```{r, echo=FALSE}
ggplot(tsla.data, aes(x = PrevClose, y = Close)) +
  geom_line(color="grey24") +
  labs(x = "previous day closing price, $", y = "closing price, $",
       title = "Tesla, Inc. closing stock prices dependence on previous data",
       subtitle = "based on data from Jan.5, 2015 until Jan.4, 2020") +
  theme(axis.ticks.y=element_blank(),
        axis.ticks.x=element_blank())
```
It seems like there actualy might be some linear relation between the values of cloe and previous close prices.  
A **linear regression** seems quite too simple model for such a case with stock prices, although we'll try it.  
  
$H_0:$ there is no linear regression pattern in the stock prices behaviour
$H_1:$ stock prices depend on the prices of the previous days and they follow a linnear regression model
```{r}
price.lm <- lm(tsla.data$Close~tsla.data$PrevClose)

summary(price.lm)
```
p-value of the F-test is almost 0 which means that we **reject $H_0$ and there is linnear relation**. Although residual standart error is quite large, as well as values of residuals statistics, to tell that model is good-fitted, as mentioned above - simple linner regression is still too simple model for this case.   

## Conclusions